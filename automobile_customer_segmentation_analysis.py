# -*- coding: utf-8 -*-
"""Automobile Customer Segmentation Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SALlrTyBKzi5W8E31VtETpUYDN0SwzkW

# **Customer Segmentation**

# Dataset Presentation

**Context:**

An automobile company has plans to enter new markets with their existing products (P1, P2, P3, P4 and P5). After intensive market research, theyâ€™ve deduced that the behavior of new market is similar to their existing market.

**Columns Description:**

* ID : Customer's ID.

* Gender : Customer's Gender (Male/Female)

* Ever_Married : Marrital Statue of the Customer (Yes/No)

* Age : Customer's Age

* Graduated : If the Customer is Graduated or No (Yes/No)

* Work_Experience : Customer's Work Experience in Years

* Spending Score : Spending Score of the Customer (Low,Average,High)

* Var_1 : Anonymised Category for the customer. ('Cat_1','Cat_2',..,'Cat_6')

* Segmentation : Category of the Customer

Link to dataset: https://www.kaggle.com/datasets/vetrirah/customer

# Connect to Google Drive
"""

from google.colab import drive
drive.mount('/content/drive')

!ls "drive/My Drive/Project/Cust_Segmen/automobile"

"""# Import Libraries"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')
# %matplotlib inline
pd.set_option('display.max_rows', 50)

# Read dataset
dir = 'drive/My Drive/Project/Cust_Segmen/automobile/Train.csv'
df = pd.read_csv(dir)
df

"""# Data Cleaning"""

# Adjusting Columns Name
new_columns = []
for i in df.columns:
    new_columns.append(i.lower().replace(' ', '_'))

df.columns = new_columns
df.columns

# Dropping Unused Columns
df.drop(columns = ['id','var_1','segmentation'] , inplace = True)

# Checking Null Values
df.isnull().sum()

# Checking Unique Values and Sum Unique Values
for col in df.columns:
    uniq = df[col].unique()
    unique_values = df[col].value_counts()
    print(f"Column: {col}")
    print(uniq)
    print("Number of Unique Values:", len(uniq))
    print(f"Sum of Each Unique Values: \n{unique_values}")
    print("---------\n")

# Filling null values with values before null values
df['ever_married'] = df['ever_married'].fillna(method='pad')
df['graduated'] = df['graduated'].fillna(method='pad')
df['work_experience'] = df['work_experience'].fillna(method='pad')
df['family_size'] = df['family_size'].fillna(method='pad')

# Dropping fields with the values which has null in profession column
df.dropna(subset = ['profession'] , inplace = True)

# Checking info each columns
df.info()

df.describe()

"""# Exploratory Data Analysis

## Exploring Categorical Feature
"""

# Exploring categorical features
df_cat = df.select_dtypes('object')
fig = plt.figure(figsize = (15,10))
plt.suptitle('Categorical features',style = 'italic' , backgroundcolor = 'skyblue', font = 'Liberation Sans')

for i,j in enumerate(df_cat.columns):
    ax = fig.add_subplot(2,3,i+1)
    ax = sns.countplot(df , x = j, palette = sns.color_palette('husl') )
    ax.set_title(j , fontsize = 15 , style = 'italic', font = 'Liberation Sans')
    ax.set_xticklabels(labels = df[j].unique() , rotation = 90)
    ax.set_ylabel('Count')
    ax.set_xlabel('')
    ax.bar_label(ax.containers[0],label_type='edge')

plt.tight_layout()

"""## Exploring Numerical Feature"""

# Exploring numerical features
df_num = df.select_dtypes(['float64','int64'])
fig = plt.figure(figsize = (15,10))
plt.suptitle('Numerical features',style = 'italic' , backgroundcolor = 'skyblue', font = 'Liberation Sans')

for i,j in enumerate(df_num.columns):
    ax = fig.add_subplot(2,3,i+1)
    ax = sns.histplot(df , x = j, kde = True )
    ax.set_title(j , fontsize = 15 , style = 'italic', font = 'Liberation Sans')
    ax.set_ylabel('Count')
    ax.set_xlabel('')

plt.tight_layout()

"""# Data Preprocessing"""

# Encoding unique values each columns
dfs=df.copy()
dfs['ever_married']=dfs['ever_married'].replace(('Yes','No'),(1,0))
dfs['graduated']=dfs['graduated'].replace(('Yes','No'),(1,0))
dfs['gender']=dfs['gender'].replace(('Male','Female'),(1,0))

proff=list(dfs['profession'].unique())
dfs['profession']=dfs['profession'].apply(lambda x:proff.index(x)+1 )

scor=list(dfs['spending_score'].unique())
dfs['spending_score']=dfs['spending_score'].apply(lambda x:scor.index(x)+1 )

dfs.head(5)

"""# Clustering"""

!pip install kmodes

from kmodes.kprototypes import KPrototypes

dfs.head(2)

cat_feature= [df.columns.get_loc(col) for col in list(df.select_dtypes('object').columns)]
data=df.values
cat_feature

kproto = KPrototypes(n_clusters=3, verbose=2, max_iter=20).fit(data, categorical=cat_feature)

"""# Cluster Analysis"""

clusters = kproto.predict(data, categorical=cat_feature)
dfs['cluster'] = list(clusters)
dfs['profession']=df['profession']
dfs['gender']=df['gender']
dfs['ever_married']=df['ever_married']
dfs['graduated']=df['graduated']
dfs['spending_score']=df['spending_score']

dfs

for i in [0,1,2]:
    plt.figure(figsize=(25,25))
    plt.subplot(3,2,i+1)
    plt.title("Cluster "+str(i))
    dfs.query("cluster=="+str(i))
    sns.countplot(dfs.query("cluster=="+str(i)),x="profession")
    plt.subplots_adjust(hspace=0.5)

for i in [0,1,2]:
    plt.figure(figsize=(25,25))
    plt.subplot(3,2,i+1)
    data = dfs.query("cluster=="+str(i)).ever_married.value_counts()


    plt.pie(data, labels=data.index, autopct='%1.1f%%')
    plt.title('Ever Married for Cluster '+str(i))
    plt.show()

for i in [0,1,2]:
    plt.figure(figsize=(25,25))
    plt.subplot(3,2,i+1)
    data = dfs.query("cluster=="+str(i)).graduated.value_counts()


    plt.pie(data, labels=data.index, autopct='%1.1f%%')
    plt.title('Graduated for Cluster '+str(i))
    plt.show()

for i in [0,1,2]:
    plt.figure(figsize=(25,25))
    plt.subplot(3,2,i+1)
    data = dfs.query("cluster=="+str(i)).spending_score.value_counts()


    plt.pie(data, labels=data.index, autopct='%1.1f%%')
    plt.title('Spending Score for Cluster '+str(i))
    plt.show()

for i in [0,1,2]:
    plt.figure(figsize=(25,25))
    plt.subplot(3,2,i+1)
    plt.title("Cluster "+str(i))
    dfs.query("cluster=="+str(i))
    sns.countplot(dfs.query("cluster=="+str(i)),x="family_size")
    plt.subplots_adjust(hspace=0.5)

print(dfs[['cluster','age']].groupby(by=['cluster']).mean())

"""# Summary

**Cluster 0**

* Jobs : Mostly Artist || Entertainment || Engineer

* Married : Yes 74 % || No 26 %

* Graduated : Yes 78 % || No 22 %

* Spending Score : Low 48% || Average 38 % || High 14 %

* Family Size : Mostly 2 || Sometimes 1 || Sometimes 3

* Average Age : 45

Cluster 2 is characterized by customers who are mostly in the artistic and executive fields, have a low and average spending score, and are mostly married and graduated, with an average age of 45.

They are likely to be budget-conscious shoppers but also willing to spend on average priced items. They are likely to have a family and are in the prime of their career.

---

**Cluster 1**

* Jobs : Mostly Healthcare || Artist || Doctor

* Married : Yes 25 % || No 75 %

* Graduated : Yes 44 % || No 56 %

* Spending Score : Low 84% || Average 11 % || High 5 %

* Family Size : Mostly 2 or 3 or 4

* Average Age : 28

Cluster 0 is characterized by customers who are mostly in the healthcare fields artistic, and doctor, have a low spending score, and are mostly not married and not graduated, with an younger average age (28).

They are mostly budget-conscious shoppers but are more likely to be single. They are likely to have a balance between work and personal life.

---

**Cluster 2**

* Jobs : Mostly Lawyer || Artist || Execution

* Married : Yes 93 % || No 7 %

* Graduated : Yes 68 % || No 32 %

* Spending Score : Low 39% || Average 23 % || High 38 %

* Family Size : Mostly 2 || Sometimes 1 || Sometimes 3

* Average Age : 70

Cluster 1 is characterized by customers who are mostly in the legal and artistic fields, have a low and high spending score, and are mostly married and graduated, with a older average age (70).

They are mostly budget-conscious shoppers and have a higher chance to be family-oriented. And also includes a high number of customers with High Spending score.
"""

